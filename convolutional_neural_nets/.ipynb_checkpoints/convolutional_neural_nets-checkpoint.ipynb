{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Nets: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Invariance**\n",
    "- Image\n",
    "    - Different positions\n",
    "    - Same objects\n",
    "- Text\n",
    "    - Kitten in a long text\n",
    "    - You can use weight sharing and train them jointly for those inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convnets**\n",
    "- Neural networks that share their parameters across space\n",
    "- ![](cn.png)\n",
    "- We take a portion of the image and run a neural network.\n",
    "    - ![](cn1.png)\n",
    "    - ![](cn2.png)\n",
    "- We then slide the neural network across the image\n",
    "    - Here you can see we've a layer that has a deeper depth but smaller space.\n",
    "    - We will slide the neural network on this layer that will again increase the depth and reduce the space.\n",
    "        - ![](cn3.png)\n",
    "    - We continue to do this until we've reached a stage of maximum depth k where k are the outputs we want. \n",
    "- Instead of having stacks of matrix multipliers, we would have stacks of convolutions.\n",
    "    - ![](cn4.png)\n",
    "        - Here you can see we're trying to reduce the space and increase the depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convnets Terms**\n",
    "- ![](cn5.png)\n",
    "- Strides\n",
    "    - ![](cn6.png)\n",
    "        - Where stride is the number of pixels that we are shifting.\n",
    "        - Stride: 1\n",
    "            - Output same size as input\n",
    "            - ![](cn7.png)\n",
    "        - Stride: 2\n",
    "            - Output roughly half the size\n",
    "            - ![](cn8.png)\n",
    "- Paddings\n",
    "    - Left: valid padding\n",
    "    - Right: same padding\n",
    "        - ![](cn9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strides, depth and padding**\n",
    "- Imagine you have 28x28 image.\n",
    "- You run a 3x3 convolution on it.\n",
    "    - Input depth: 3\n",
    "    - Output depth: 8\n",
    "- ![](cn10.png)\n",
    "    - For stride: 1 and padding: same (1)\n",
    "        - You would have the exact same dimensions.\n",
    "        - You would be taking a F x F x D_input dot-product to come up with a number.\n",
    "    - For stride: 1 and padding: valid (0)\n",
    "        - You would have one less row and column\n",
    "    - For stride: 2 and padding: valid (0)\n",
    "        - You would have half the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating Output Size**\n",
    "- $O = \\frac {W - K - 2P} {S} + 1 $\n",
    "    - O is the output height/length\n",
    "    - W is the input height/length\n",
    "    - K is the filter size (kernel size)\n",
    "    - P is the padding\n",
    "    - S is the stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding Size**\n",
    "- In general it's common to see same (zero) padding, stride 1 and filters of size FxF.\n",
    "- Zero-padding = $\\frac {F - 1}{2}$\n",
    "    - If you do not pad (same padding), you would decrease the width and height of your layers gradually. \n",
    "        - This might not be something you want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Depth**\n",
    "- Number of filters = depth.\n",
    "- We try to keep this in powers of two. \n",
    "    - 32, 64, 128, 512 etc.\n",
    "    - This is for computational reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Parameters**\n",
    "- Number of parameters in layer = (F x F x D_input + 1) x D_filter\n",
    "    - Where F is the filter size\n",
    "    - D_input is the depth of the input layer\n",
    "    - 1 is the bias\n",
    "    - D_filter is the depth of the filter\n",
    "    - Parameters per filter: (F x F x D_input + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolution Networks**\n",
    "- ![](cn11.png)\n",
    "- ![](cn12.png)\n",
    "    - **Fully Connected (FC) Layer**\n",
    "        - Basically it connects to the entire input volume like a neural network. \n",
    "        - Final layer after we have done all our convolutions.\n",
    "    - **ReLU Layers**\n",
    "        - Remember there are ReLU Layers after every Conv and FC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced convnet-ology**\n",
    "- Pooling\n",
    "- 1 x 1 convolutions\n",
    "- Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pooling**\n",
    "- Striding\n",
    "    - We shift the filter by a few pixel each time.\n",
    "    - This is very aggressive method that removes a lot of information.\n",
    "- Pooling\n",
    "    - We can take a smaller stride.\n",
    "    - Take all the convolutions in the neighbors.\n",
    "    - Combine them somehow, and this is called pooling.\n",
    "        - We will be preserving the depth.\n",
    "        - But we will be reducing the width and height.\n",
    "        - ![](cn13.png)\n",
    "- 1. Max Pooling\n",
    "    - At every point in a feature map, look at a small neighborhood around that point and compute the maximum of all the responses around it.\n",
    "    - ![](cn14.png)\n",
    "    - Typical architecture\n",
    "        - ![](cn15.png)\n",
    "- 2. Average pooling\n",
    "    - Instead oftaking the max, we take the average.\n",
    "    - It's similar to taking a blurred, low-resolution, view of the feature map.\n",
    "        - ![](cn16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1x1 Convolutions**\n",
    "- Here we are using only 1 pixel by 1 pixel.\n",
    "- Traditional.\n",
    "    - ![](cn17.png)\n",
    "- Now we add a 1x1 convolution.\n",
    "    - ![](cn18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inception Module**\n",
    "- This is like an ensemble of methods.\n",
    "- ![](cn19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation of results**\n",
    "- We can use accuracy to evaluate the predicted values and our labels.\n",
    "- But a better method would be to use the top-1 and top-5 errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Progress in Convs (in order of lower top-1 and top-5 errors)**\n",
    "- LeNet-5\n",
    "- AlexNet\n",
    "- ZFNet\n",
    "- VGGNet\n",
    "- GoogLeNet\n",
    "- ResNet\n",
    "    - As of September 2016, this is the latest state-of-the-art implementation for convs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Readings**\n",
    "- [Convolution Arithmetic for Deep Learning](http://arxiv.org/pdf/1603.07285v1.pdf)\n",
    "- [A Beginner’s Guide To Understanding Convolutional Neural Networks Part 1](http://www.kdnuggets.com/2016/09/beginners-guide-understanding-convolutional-neural-networks-part-1.html)\n",
    "- [A Beginner’s Guide To Understanding Convolutional Neural Networks Part 2](http://www.kdnuggets.com/2016/09/beginners-guide-understanding-convolutional-neural-networks-part-2.html)\n",
    "- [CS231n Winter 2016 Lecture 7 Convolutional Neural Networks Video](https://www.youtube.com/watch?v=AQirPKrAyDg)\n",
    "- [CS231n Winter 2016 Lecture 7 Convolutional Neural Networks Lecture Notes](http://cs231n.stanford.edu/slides/winter1516_lecture7.pdf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
